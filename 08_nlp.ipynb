{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from helper_funcs import create_tensorboard_callback, create_checkpoint_callback, plot_loss_curves, compare_historys, unzip_data, calculate_results\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  593k  100  593k    0     0  1814k      0 --:--:-- --:--:-- --:--:-- 1813k\n"
     ]
    }
   ],
   "source": [
    "# !wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
    "!curl -O https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
    "unzip_data(\"nlp_getting_started.zip\", path_name='nlp_getting_started/')\n",
    "!rm nlp_getting_started.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('nlp_getting_started/train.csv').sample(frac=1, random_state=42)\n",
    "test_df = pd.read_csv('nlp_getting_started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 3048\n",
      "Target: 1,  (disaster)\n",
      "Text:'There was a small earthquake in LA but don't worry Emily Rossum is fine' #difficultpeople is great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_index = random.randint(0, len(train_df))\n",
    "target = train_df['target'][random_index]\n",
    "print(f\"Index: {random_index}\")\n",
    "print(f\"Target: {target}, {' (disaster)' if(target == 1) else ' (not a real disaster)'}\")\n",
    "print(f\"Text:{train_df['text'][random_index]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df['text'].to_numpy(), train_df['target'].to_numpy(), test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 762, 6851, 762)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(val_sentences), len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 15:31:16.739491: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-01-18 15:31:16.739518: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-01-18 15:31:16.739527: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-01-18 15:31:16.739591: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-18 15:31:16.739825: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-01-18 15:31:16.854103: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "text_vectorization_default = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=None,\n",
    ")\n",
    "text_vectorization_default.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21056"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vectorization_default.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(i.split()) for i in train_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorization([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@lizbon @KidicalMassDC It's more of a structural breakdown. Or maybe a patience failure on their part.        \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[  1,   1,  37,  51,   6,   3, 384,   1,  53, 680,   3, 999, 320,\n",
       "         11, 131]])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "        \\n\\nVectorized version:\")\n",
    "text_vectorization([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I entered to #win the ENTIRE set of butterLONDON Lip Crayons via @be_ram0s. - Go enter! #bbloggers http://t.co/DsB3lDfuxU        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.04233202, -0.03212961,  0.00704069, ..., -0.04695761,\n",
       "          0.02042797, -0.02041524],\n",
       "        [ 0.0066098 ,  0.00506943, -0.03385796, ...,  0.02842617,\n",
       "          0.03705411, -0.04995061],\n",
       "        [ 0.02861372,  0.04672357, -0.02850316, ..., -0.02490436,\n",
       "         -0.00039848, -0.00875305],\n",
       "        ...,\n",
       "        [-0.02548779, -0.02951611,  0.04010404, ...,  0.0011493 ,\n",
       "          0.01746242, -0.03864621],\n",
       "        [ 0.04847072, -0.03766227, -0.0496028 , ..., -0.02253153,\n",
       "          0.03967238, -0.04563466],\n",
       "        [ 0.01242206, -0.03003116,  0.03178943, ..., -0.00290171,\n",
       "          0.00995159, -0.02689688]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "        \\n\\nEmbedded version:\")\n",
    "embedding(text_vectorization([random_sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Naive Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB()),\n",
    "])\n",
    "\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7926509186351706"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.score(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_result = calculate_results(val_labels, baseline_preds)\n",
    "model_0_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_1/20240118-153246\n",
      "Saving model checkpoints to: checkpoints/08_model_1/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 23ms/step - loss: 0.6147 - accuracy: 0.6871 - val_loss: 0.5397 - val_accuracy: 0.7533\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4439 - accuracy: 0.8168 - val_loss: 0.4696 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.3483 - accuracy: 0.8612 - val_loss: 0.4592 - val_accuracy: 0.7913\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.2848 - accuracy: 0.8901 - val_loss: 0.4648 - val_accuracy: 0.7900\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.2384 - accuracy: 0.9137 - val_loss: 0.4774 - val_accuracy: 0.7822\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorization(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")\n",
    "\n",
    "model_1.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_1 = model_1.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_1\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_1\"),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4796 - accuracy: 0.7861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4795877933502197, 0.7860892415046692]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.377892  ],\n",
       "       [0.6727631 ],\n",
       "       [0.9976393 ],\n",
       "       [0.10367664],\n",
       "       [0.17349124],\n",
       "       [0.93533397],\n",
       "       [0.91008604],\n",
       "       [0.9928075 ],\n",
       "       [0.9630222 ],\n",
       "       [0.26276284]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.60892388451444,\n",
       " 'precision': 0.7907394181632303,\n",
       " 'recall': 0.7860892388451444,\n",
       " 'f1': 0.7831536805930754}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_result = calculate_results(val_labels, model_1_preds)\n",
    "model_1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_weights = model_1.layers[2].get_weights()[0]\n",
    "embed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'a',\n",
       " 'in',\n",
       " 'to',\n",
       " 'of',\n",
       " 'and',\n",
       " 'i',\n",
       " 'is',\n",
       " 'for',\n",
       " 'on',\n",
       " 'you',\n",
       " 'my',\n",
       " 'with',\n",
       " 'it',\n",
       " 'that',\n",
       " 'at',\n",
       " 'by',\n",
       " 'this',\n",
       " 'from',\n",
       " 'be',\n",
       " 'are',\n",
       " 'was',\n",
       " 'have',\n",
       " 'like',\n",
       " 'as',\n",
       " 'up',\n",
       " 'so',\n",
       " 'just',\n",
       " 'but',\n",
       " 'me',\n",
       " 'im',\n",
       " 'your',\n",
       " 'not',\n",
       " 'amp',\n",
       " 'out',\n",
       " 'its',\n",
       " 'will',\n",
       " 'an',\n",
       " 'no',\n",
       " 'has',\n",
       " 'fire',\n",
       " 'after',\n",
       " 'all',\n",
       " 'when',\n",
       " 'we',\n",
       " 'if',\n",
       " 'now',\n",
       " 'via',\n",
       " 'new',\n",
       " 'more',\n",
       " 'get',\n",
       " 'or',\n",
       " 'about',\n",
       " 'what',\n",
       " 'he',\n",
       " 'people',\n",
       " 'news',\n",
       " 'been',\n",
       " 'over',\n",
       " 'one',\n",
       " 'how',\n",
       " 'dont',\n",
       " 'they',\n",
       " 'who',\n",
       " 'into',\n",
       " 'were',\n",
       " 'do',\n",
       " 'us',\n",
       " '2',\n",
       " 'can',\n",
       " 'video',\n",
       " 'emergency',\n",
       " 'there',\n",
       " 'disaster',\n",
       " 'than',\n",
       " 'police',\n",
       " 'would',\n",
       " 'his',\n",
       " 'still',\n",
       " 'her',\n",
       " 'some',\n",
       " 'body',\n",
       " 'storm',\n",
       " 'crash',\n",
       " 'burning',\n",
       " 'suicide',\n",
       " 'back',\n",
       " 'man',\n",
       " 'california',\n",
       " 'why',\n",
       " 'time',\n",
       " 'them',\n",
       " 'had',\n",
       " 'buildings',\n",
       " 'rt',\n",
       " 'first',\n",
       " 'cant',\n",
       " 'see',\n",
       " 'got',\n",
       " 'day',\n",
       " 'off',\n",
       " 'our',\n",
       " 'going',\n",
       " 'nuclear',\n",
       " 'know',\n",
       " 'world',\n",
       " 'bomb',\n",
       " 'fires',\n",
       " 'love',\n",
       " 'killed',\n",
       " 'go',\n",
       " 'attack',\n",
       " 'youtube',\n",
       " 'dead',\n",
       " 'two',\n",
       " 'families',\n",
       " '3',\n",
       " 'train',\n",
       " 'full',\n",
       " 'being',\n",
       " 'war',\n",
       " 'many',\n",
       " 'today',\n",
       " 'think',\n",
       " 'only',\n",
       " 'car',\n",
       " 'accident',\n",
       " 'life',\n",
       " 'hiroshima',\n",
       " 'their',\n",
       " 'say',\n",
       " 'may',\n",
       " 'down',\n",
       " 'watch',\n",
       " 'good',\n",
       " 'could',\n",
       " 'want',\n",
       " 'last',\n",
       " 'here',\n",
       " 'years',\n",
       " 'u',\n",
       " 'then',\n",
       " 'make',\n",
       " 'did',\n",
       " 'wildfire',\n",
       " 'way',\n",
       " 'help',\n",
       " 'best',\n",
       " 'too',\n",
       " 'even',\n",
       " 'because',\n",
       " 'home',\n",
       " 'death',\n",
       " 'collapse',\n",
       " 'bombing',\n",
       " 'mass',\n",
       " 'him',\n",
       " 'black',\n",
       " 'am',\n",
       " 'those',\n",
       " 'need',\n",
       " 'fatal',\n",
       " 'army',\n",
       " 'another',\n",
       " 'work',\n",
       " 'take',\n",
       " 'should',\n",
       " 'really',\n",
       " 'please',\n",
       " 'mh370',\n",
       " 'youre',\n",
       " 'look',\n",
       " 'lol',\n",
       " 'hot',\n",
       " 'pm',\n",
       " 'legionnaires',\n",
       " '4',\n",
       " 'right',\n",
       " '5',\n",
       " 'let',\n",
       " 'city',\n",
       " 'year',\n",
       " 'wreck',\n",
       " 'school',\n",
       " 'northern',\n",
       " 'much',\n",
       " 'forest',\n",
       " 'bomber',\n",
       " 'water',\n",
       " 'she',\n",
       " 'never',\n",
       " 'read',\n",
       " 'latest',\n",
       " 'homes',\n",
       " 'great',\n",
       " 'every',\n",
       " '1',\n",
       " 'live',\n",
       " 'god',\n",
       " 'fear',\n",
       " 'any',\n",
       " '\\x89Û',\n",
       " 'under',\n",
       " 'said',\n",
       " 'old',\n",
       " 'floods',\n",
       " '2015',\n",
       " 'getting',\n",
       " 'atomic',\n",
       " 'while',\n",
       " 'top',\n",
       " 'obama',\n",
       " 'feel',\n",
       " 'thats',\n",
       " 'since',\n",
       " 'near',\n",
       " 'flames',\n",
       " 'ever',\n",
       " 'come',\n",
       " 'where',\n",
       " 'these',\n",
       " 'military',\n",
       " 'japan',\n",
       " 'found',\n",
       " 'content',\n",
       " 'ass',\n",
       " 'without',\n",
       " 'weather',\n",
       " 'most',\n",
       " 'flooding',\n",
       " 'flood',\n",
       " 'damage',\n",
       " 'which',\n",
       " 'shit',\n",
       " 's',\n",
       " 'hope',\n",
       " 'everyone',\n",
       " 'before',\n",
       " 'stop',\n",
       " 'plan',\n",
       " 'malaysia',\n",
       " 'injured',\n",
       " 'hit',\n",
       " 'evacuation',\n",
       " 'during',\n",
       " 'debris',\n",
       " 'cross',\n",
       " 'coming',\n",
       " 'wild',\n",
       " 'well',\n",
       " 'times',\n",
       " 'sinking',\n",
       " 'oil',\n",
       " 'fucking',\n",
       " 'check',\n",
       " 'cause',\n",
       " 'weapons',\n",
       " 'truck',\n",
       " 'food',\n",
       " 'bloody',\n",
       " 'always',\n",
       " 'weapon',\n",
       " 'theres',\n",
       " 'state',\n",
       " 'little',\n",
       " 'injuries',\n",
       " 'free',\n",
       " 'wounded',\n",
       " 'summer',\n",
       " 'smoke',\n",
       " 'severe',\n",
       " 'reddit',\n",
       " 'next',\n",
       " 'movie',\n",
       " 'ive',\n",
       " 'hes',\n",
       " 'fall',\n",
       " 'evacuate',\n",
       " 'confirmed',\n",
       " 'bad',\n",
       " 'again',\n",
       " 'thunderstorm',\n",
       " 'set',\n",
       " 'night',\n",
       " 'natural',\n",
       " 'looks',\n",
       " 'heat',\n",
       " 'face',\n",
       " 'earthquake',\n",
       " 'boy',\n",
       " 'whole',\n",
       " 'until',\n",
       " 'thunder',\n",
       " 'through',\n",
       " 'says',\n",
       " 'panic',\n",
       " 'outbreak',\n",
       " 'made',\n",
       " 'lightning',\n",
       " 'fatalities',\n",
       " 'family',\n",
       " 'explosion',\n",
       " 'end',\n",
       " 'destroy',\n",
       " 'derailment',\n",
       " 'air',\n",
       " 'w',\n",
       " 'terrorist',\n",
       " 'survive',\n",
       " 'screaming',\n",
       " 'saudi',\n",
       " 'refugees',\n",
       " 'rain',\n",
       " 'murder',\n",
       " 'loud',\n",
       " 'liked',\n",
       " 'house',\n",
       " 'gonna',\n",
       " 'failure',\n",
       " 'collided',\n",
       " 'bag',\n",
       " 'attacked',\n",
       " 'ambulance',\n",
       " '70',\n",
       " 'wind',\n",
       " 'services',\n",
       " 'save',\n",
       " 'report',\n",
       " 'migrants',\n",
       " 'head',\n",
       " 'explode',\n",
       " 'charged',\n",
       " 'change',\n",
       " 'big',\n",
       " 'also',\n",
       " 'wrecked',\n",
       " 'warning',\n",
       " 'update',\n",
       " 'run',\n",
       " 'rescuers',\n",
       " 'released',\n",
       " 'photo',\n",
       " 'massacre',\n",
       " 'injury',\n",
       " 'hurricane',\n",
       " 'high',\n",
       " 'hail',\n",
       " 'fuck',\n",
       " 'does',\n",
       " 'destroyed',\n",
       " 'bus',\n",
       " 'blood',\n",
       " '40',\n",
       " '\\x89ÛÒ',\n",
       " 'wreckage',\n",
       " 'violent',\n",
       " 'twister',\n",
       " 'trauma',\n",
       " 'tragedy',\n",
       " 'terrorism',\n",
       " 'survivors',\n",
       " 'survived',\n",
       " 'sinkhole',\n",
       " 'sandstorm',\n",
       " 'road',\n",
       " 'rioting',\n",
       " 'red',\n",
       " 'real',\n",
       " 'put',\n",
       " 'post',\n",
       " 'national',\n",
       " 'missing',\n",
       " 'landslide',\n",
       " 'keep',\n",
       " 'girl',\n",
       " 'drought',\n",
       " 'curfew',\n",
       " 'breaking',\n",
       " 'bags',\n",
       " 'white',\n",
       " 'twitter',\n",
       " 'tonight',\n",
       " 'structural',\n",
       " 'spill',\n",
       " 'service',\n",
       " 'screamed',\n",
       " 'rescued',\n",
       " 'rescue',\n",
       " 'phone',\n",
       " 'ok',\n",
       " 'oh',\n",
       " 'mosque',\n",
       " 'lives',\n",
       " 'horrible',\n",
       " 'harm',\n",
       " 'game',\n",
       " 'dust',\n",
       " 'destruction',\n",
       " 'deluge',\n",
       " 'deaths',\n",
       " 'crashed',\n",
       " 'cliff',\n",
       " 'catastrophe',\n",
       " 'boat',\n",
       " 'away',\n",
       " 'august',\n",
       " 'area',\n",
       " 'apocalypse',\n",
       " 'woman',\n",
       " 'whirlwind',\n",
       " 'traumatised',\n",
       " 'stock',\n",
       " 'saw',\n",
       " 'ruin',\n",
       " 'riot',\n",
       " 'quarantine',\n",
       " 'kills',\n",
       " 'island',\n",
       " 'investigators',\n",
       " 'ill',\n",
       " 'hostages',\n",
       " 'hazard',\n",
       " 'danger',\n",
       " 'call',\n",
       " '15',\n",
       " 'women',\n",
       " 'windstorm',\n",
       " 'things',\n",
       " 'suspect',\n",
       " 'show',\n",
       " 'reunion',\n",
       " 'quarantined',\n",
       " 'lava',\n",
       " 'heart',\n",
       " 'engulfed',\n",
       " 'detonate',\n",
       " 'crush',\n",
       " 'collapsed',\n",
       " 'came',\n",
       " 'better',\n",
       " 'battle',\n",
       " 'armageddon',\n",
       " 'airplane',\n",
       " 'against',\n",
       " 'affected',\n",
       " 'use',\n",
       " 'trapped',\n",
       " 'thank',\n",
       " 'sunk',\n",
       " 'story',\n",
       " 'send',\n",
       " 'part',\n",
       " 'other',\n",
       " 'must',\n",
       " 'mudslide',\n",
       " 'market',\n",
       " 'iran',\n",
       " 'famine',\n",
       " 'exploded',\n",
       " 'electrocuted',\n",
       " 'ebay',\n",
       " 'displaced',\n",
       " 'derailed',\n",
       " 'derail',\n",
       " 'burned',\n",
       " 'bombed',\n",
       " 'blown',\n",
       " 'baby',\n",
       " 'around',\n",
       " 'zone',\n",
       " 'wave',\n",
       " 'wanna',\n",
       " 'sure',\n",
       " 'someone',\n",
       " 'screams',\n",
       " 'razed',\n",
       " 'power',\n",
       " 'obliterated',\n",
       " 'long',\n",
       " 'land',\n",
       " 'hundreds',\n",
       " 'heard',\n",
       " 'group',\n",
       " 'flattened',\n",
       " 'drown',\n",
       " 'doing',\n",
       " 'care',\n",
       " 'bridge',\n",
       " 'bagging',\n",
       " '9',\n",
       " 'went',\n",
       " 'used',\n",
       " 'typhoon',\n",
       " 'trouble',\n",
       " 'tornado',\n",
       " 'thought',\n",
       " 'thing',\n",
       " 'river',\n",
       " 'responders',\n",
       " 'past',\n",
       " 'pandemonium',\n",
       " 'officials',\n",
       " 'meltdown',\n",
       " 'lot',\n",
       " 'least',\n",
       " 'inundated',\n",
       " 'id',\n",
       " 'hostage',\n",
       " 'hijacking',\n",
       " 'hazardous',\n",
       " 'goes',\n",
       " 'drowning',\n",
       " 'didnt',\n",
       " 'devastation',\n",
       " 'demolish',\n",
       " 'collide',\n",
       " 'casualties',\n",
       " 'calgary',\n",
       " 'bang',\n",
       " 'anniversary',\n",
       " 'yet',\n",
       " 'wounds',\n",
       " 'volcano',\n",
       " 'tsunami',\n",
       " 'sue',\n",
       " 'st',\n",
       " 'song',\n",
       " 'something',\n",
       " 'shoulder',\n",
       " 'security',\n",
       " 'prebreak',\n",
       " 'possible',\n",
       " 'pkk',\n",
       " 'panicking',\n",
       " 'obliteration',\n",
       " 'obliterate',\n",
       " 'murderer',\n",
       " 'minute',\n",
       " 'light',\n",
       " 'lets',\n",
       " 'kill',\n",
       " 'isis',\n",
       " 'india',\n",
       " 'hijacker',\n",
       " 'hellfire',\n",
       " 'government',\n",
       " 'few',\n",
       " 'evacuated',\n",
       " 'due',\n",
       " 'detonated',\n",
       " 'desolation',\n",
       " 'crushed',\n",
       " 'chemical',\n",
       " 'blew',\n",
       " 'blazing',\n",
       " 'blast',\n",
       " 'annihilated',\n",
       " 'airport',\n",
       " '6',\n",
       " 'week',\n",
       " 'upheaval',\n",
       " 'trying',\n",
       " 'three',\n",
       " 'thanks',\n",
       " 'sound',\n",
       " 'soon',\n",
       " 'sirens',\n",
       " 'rainstorm',\n",
       " 'plane',\n",
       " 'music',\n",
       " 'making',\n",
       " 'kids',\n",
       " 'issues',\n",
       " 'half',\n",
       " 'guys',\n",
       " 'fedex',\n",
       " 'done',\n",
       " 'died',\n",
       " 'detonation',\n",
       " 'days',\n",
       " 'cyclone',\n",
       " 'county',\n",
       " 'collision',\n",
       " 'caused',\n",
       " 'catastrophic',\n",
       " 'bleeding',\n",
       " 'beautiful',\n",
       " '8',\n",
       " 'words',\n",
       " 'very',\n",
       " 'traffic',\n",
       " 'south',\n",
       " 'remember',\n",
       " 'policy',\n",
       " 'place',\n",
       " 'nothing',\n",
       " 'north',\n",
       " 'mp',\n",
       " 'longer',\n",
       " 'left',\n",
       " 'israeli',\n",
       " 'hell',\n",
       " 'fun',\n",
       " 'drowned',\n",
       " 'demolished',\n",
       " 'cool',\n",
       " 'both',\n",
       " 'bioterror',\n",
       " 'believe',\n",
       " 'avalanche',\n",
       " 'arson',\n",
       " 'turkey',\n",
       " 'snowstorm',\n",
       " 'site',\n",
       " 'shot',\n",
       " 'shooting',\n",
       " 'pic',\n",
       " 'nowplaying',\n",
       " 'media',\n",
       " 'islam',\n",
       " 'inside',\n",
       " 'hijack',\n",
       " 'helicopter',\n",
       " 'fight',\n",
       " 'fatality',\n",
       " 'fan',\n",
       " 'electrocute',\n",
       " 'doesnt',\n",
       " 'building',\n",
       " 'brown',\n",
       " 'bc',\n",
       " 'actually',\n",
       " '16yr',\n",
       " 'yes',\n",
       " 'watching',\n",
       " 'wait',\n",
       " 'ur',\n",
       " 'tell',\n",
       " 'swallowed',\n",
       " 'seismic',\n",
       " 'second',\n",
       " 'rubble',\n",
       " 're\\x89Û',\n",
       " 'plans',\n",
       " 'men',\n",
       " 'memories',\n",
       " 'line',\n",
       " 'la',\n",
       " 'horror',\n",
       " 'health',\n",
       " 'having',\n",
       " 'find',\n",
       " 'eyewitness',\n",
       " 'deluged',\n",
       " 'children',\n",
       " 'bush',\n",
       " 'anything',\n",
       " 'already',\n",
       " 'almost',\n",
       " 'aircraft',\n",
       " 'yourself',\n",
       " 'yeah',\n",
       " 'whats',\n",
       " 'tomorrow',\n",
       " 'such',\n",
       " 'start',\n",
       " 'side',\n",
       " 'searching',\n",
       " 'saved',\n",
       " 'reactor',\n",
       " 'probably',\n",
       " 'play',\n",
       " 'person',\n",
       " 'peace',\n",
       " 'outside',\n",
       " 'officer',\n",
       " 'nearby',\n",
       " 'n',\n",
       " 'maybe',\n",
       " 'lost',\n",
       " 'literally',\n",
       " 'hours',\n",
       " 'hear',\n",
       " 'far',\n",
       " 'die',\n",
       " 'demolition',\n",
       " 'data',\n",
       " 'crews',\n",
       " 'conclusively',\n",
       " 'business',\n",
       " 'american',\n",
       " '20',\n",
       " '\\x89ÛÓ',\n",
       " 'west',\n",
       " 'waves',\n",
       " 'team',\n",
       " 'street',\n",
       " 'stay',\n",
       " 'soudelor',\n",
       " 'reuters',\n",
       " 'manslaughter',\n",
       " 'leather',\n",
       " 'job',\n",
       " 'history',\n",
       " 'hey',\n",
       " 'feeling',\n",
       " 'eyes',\n",
       " 'everything',\n",
       " 'declares',\n",
       " 'deal',\n",
       " 'casualty',\n",
       " 'bodies',\n",
       " 'amid',\n",
       " 'ablaze',\n",
       " '7',\n",
       " '50',\n",
       " '30',\n",
       " '12',\n",
       " 'youth',\n",
       " 'wont',\n",
       " 'wake',\n",
       " 'theyre',\n",
       " 'support',\n",
       " 'stretcher',\n",
       " 'same',\n",
       " 'rise',\n",
       " 'picking',\n",
       " 'photos',\n",
       " 'own',\n",
       " 'others',\n",
       " 'order',\n",
       " 'omg',\n",
       " 'okay',\n",
       " 'name',\n",
       " 'myself',\n",
       " 'money',\n",
       " 'makes',\n",
       " 'leave',\n",
       " 'lab',\n",
       " 'gt',\n",
       " 'gets',\n",
       " 'flag',\n",
       " 'desolate',\n",
       " 'crisis',\n",
       " 'center',\n",
       " 'book',\n",
       " 'blight',\n",
       " 'blaze',\n",
       " 'ago',\n",
       " 'abc',\n",
       " '11yearold',\n",
       " 'womens',\n",
       " 'typhoondevastated',\n",
       " 'tv',\n",
       " 'trench',\n",
       " 'trains',\n",
       " 'texas',\n",
       " 'space',\n",
       " 'siren',\n",
       " 'shes',\n",
       " 'self',\n",
       " 'saipan',\n",
       " 'reason',\n",
       " 'rd',\n",
       " 'pretty',\n",
       " 'pick',\n",
       " 'offensive',\n",
       " 'move',\n",
       " 'meek',\n",
       " 'major',\n",
       " 'm',\n",
       " 'low',\n",
       " 'lord',\n",
       " 'huge',\n",
       " 'hat',\n",
       " 'flash',\n",
       " 'feared',\n",
       " 'fast',\n",
       " 'effect',\n",
       " 'course',\n",
       " 'country',\n",
       " 'control',\n",
       " 'class',\n",
       " 'child',\n",
       " 'chance',\n",
       " 'caught',\n",
       " 'called',\n",
       " 'bioterrorism',\n",
       " 'bestnaijamade',\n",
       " 'become',\n",
       " 'bar',\n",
       " 'banned',\n",
       " 'ball',\n",
       " 'aug',\n",
       " 'annihilation',\n",
       " 'wrong',\n",
       " 'win',\n",
       " 'usa',\n",
       " 'united',\n",
       " 'town',\n",
       " 'totally',\n",
       " 'toddler',\n",
       " 'though',\n",
       " 'temple',\n",
       " 'taken',\n",
       " 'stand',\n",
       " 'spot',\n",
       " 'signs',\n",
       " 'ship',\n",
       " 'pakistan',\n",
       " 'online',\n",
       " 'level',\n",
       " 'ladies',\n",
       " 'jobs',\n",
       " 'isnt',\n",
       " 'happy',\n",
       " 'hailstorm',\n",
       " 'friends',\n",
       " 'disea',\n",
       " 'damn',\n",
       " 'couple',\n",
       " 'case',\n",
       " 'blue',\n",
       " 'bigger',\n",
       " 'america',\n",
       " 'across',\n",
       " '10',\n",
       " 'yours',\n",
       " 'village',\n",
       " 'try',\n",
       " 'transport',\n",
       " 'talk',\n",
       " 'seen',\n",
       " 'russian',\n",
       " 'radio',\n",
       " 'projected',\n",
       " 'once',\n",
       " 'official',\n",
       " 'needs',\n",
       " 'nearly',\n",
       " 'mount',\n",
       " 'might',\n",
       " 'mayhem',\n",
       " 'instead',\n",
       " 'hollywood',\n",
       " 'haha',\n",
       " 'guy',\n",
       " 'gun',\n",
       " 'green',\n",
       " 'front',\n",
       " 'finally',\n",
       " 'favorite',\n",
       " 'experts',\n",
       " 'entire',\n",
       " 'east',\n",
       " 'daily',\n",
       " 'crazy',\n",
       " 'computers',\n",
       " 'coaches',\n",
       " 'christian',\n",
       " 'china',\n",
       " 'blizzard',\n",
       " 'anyone',\n",
       " 'aint',\n",
       " 'action',\n",
       " '25',\n",
       " 'virgin',\n",
       " 'vehicle',\n",
       " 'truth',\n",
       " 'trust',\n",
       " 'takes',\n",
       " 't',\n",
       " 'star',\n",
       " 'sorry',\n",
       " 'running',\n",
       " 'refugio',\n",
       " 'reddits',\n",
       " 'poor',\n",
       " 'pain',\n",
       " 'mom',\n",
       " 'miners',\n",
       " 'marks',\n",
       " 'looking',\n",
       " 'knock',\n",
       " 'issued',\n",
       " 'insurance',\n",
       " 'ignition',\n",
       " 'houses',\n",
       " 'heavy',\n",
       " 'hate',\n",
       " 'hard',\n",
       " 'happened',\n",
       " 'global',\n",
       " 'giant',\n",
       " 'gbbo',\n",
       " 'flight',\n",
       " 'eye',\n",
       " 'emmerdale',\n",
       " 'driver',\n",
       " 'devastated',\n",
       " 'd',\n",
       " 'costlier',\n",
       " 'cnn',\n",
       " 'cars',\n",
       " 'camp',\n",
       " 'beach',\n",
       " 'arsonist',\n",
       " 'angry',\n",
       " 'alone',\n",
       " 'added',\n",
       " '05',\n",
       " 'york',\n",
       " 'wonder',\n",
       " 'uk',\n",
       " 'turn',\n",
       " 'taking',\n",
       " 'subreddits',\n",
       " 'sounds',\n",
       " 'scared',\n",
       " 'russia',\n",
       " 'rly',\n",
       " 'reports',\n",
       " 'ready',\n",
       " 'quiz',\n",
       " 'public',\n",
       " 'property',\n",
       " 'pradesh',\n",
       " 'ppl',\n",
       " 'playing',\n",
       " 'pay',\n",
       " 'parole',\n",
       " 'pamela',\n",
       " 'pakistani',\n",
       " 'outrage',\n",
       " 'niggas',\n",
       " 'nagasaki',\n",
       " 'myanmar',\n",
       " 'muslims',\n",
       " 'mop',\n",
       " 'madhya',\n",
       " 'mad',\n",
       " 'lmao',\n",
       " 'learn',\n",
       " 'large',\n",
       " 'govt',\n",
       " 'give',\n",
       " 'gems',\n",
       " 'gave',\n",
       " 'funtenna',\n",
       " 'fukushima',\n",
       " 'former',\n",
       " 'film',\n",
       " 'earth',\n",
       " 'drive',\n",
       " 'downtown',\n",
       " 'dog',\n",
       " 'comes',\n",
       " 'closed',\n",
       " 'cake',\n",
       " 'british',\n",
       " 'bring',\n",
       " 'bbc',\n",
       " 'b',\n",
       " 'appears',\n",
       " 'aftershock',\n",
       " '13',\n",
       " '11',\n",
       " 'young',\n",
       " 'wow',\n",
       " 'worst',\n",
       " 'waving',\n",
       " 'washington',\n",
       " 'wanted',\n",
       " 'vs',\n",
       " 'view',\n",
       " 'upon',\n",
       " 'tweet',\n",
       " 'tree',\n",
       " 'tote',\n",
       " 'thousands',\n",
       " 'thinking',\n",
       " 'theater',\n",
       " 'soul',\n",
       " 'sky',\n",
       " 'sign',\n",
       " 'shows',\n",
       " 'shift',\n",
       " 'seeing',\n",
       " 'sea',\n",
       " 'scene',\n",
       " 'safety',\n",
       " 'rules',\n",
       " 'rock',\n",
       " 'reported',\n",
       " 'r',\n",
       " 'pray',\n",
       " 'playlist',\n",
       " 'patience',\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = text_vectorizer.get_vocabulary()\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocabulary):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_2/20240118-145002\n",
      "Saving model checkpoints to: checkpoints/08_model_2/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 22ms/step - loss: 0.5119 - accuracy: 0.7422 - val_loss: 0.4611 - val_accuracy: 0.7835\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.3161 - accuracy: 0.8717 - val_loss: 0.4830 - val_accuracy: 0.7782\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2139 - accuracy: 0.9186 - val_loss: 0.5651 - val_accuracy: 0.7743\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1453 - accuracy: 0.9473 - val_loss: 0.7196 - val_accuracy: 0.7808\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1025 - accuracy: 0.9631 - val_loss: 0.9998 - val_accuracy: 0.7651\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.LSTM(64)(x)\n",
    "# x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n",
    "\n",
    "model_2.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_2 = model_2.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_2\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_2\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00574437],\n",
       "       [0.782921  ],\n",
       "       [0.9996877 ],\n",
       "       [0.0323944 ],\n",
       "       [0.00319531],\n",
       "       [0.9997781 ],\n",
       "       [0.96810436],\n",
       "       [0.99984884],\n",
       "       [0.9997718 ],\n",
       "       [0.12086073]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.50918635170603,\n",
       " 'precision': 0.7664434345240916,\n",
       " 'recall': 0.7650918635170604,\n",
       " 'f1': 0.7630272521222509}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_result = calculate_results(val_labels, model_2_preds)\n",
    "model_2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_3/20240118-145022\n",
      "Saving model checkpoints to: checkpoints/08_model_3/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 25ms/step - loss: 0.5393 - accuracy: 0.7209 - val_loss: 0.4577 - val_accuracy: 0.7874\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3221 - accuracy: 0.8721 - val_loss: 0.4664 - val_accuracy: 0.7940\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.2147 - accuracy: 0.9215 - val_loss: 0.5564 - val_accuracy: 0.7651\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1579 - accuracy: 0.9451 - val_loss: 0.5804 - val_accuracy: 0.7835\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1149 - accuracy: 0.9609 - val_loss: 0.6626 - val_accuracy: 0.7703\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.GRU(64)(x)\n",
    "# x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")\n",
    "\n",
    "model_3.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_3 = model_3.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_3\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_3\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6367205 ],\n",
       "       [0.9633442 ],\n",
       "       [0.9949713 ],\n",
       "       [0.03364241],\n",
       "       [0.00682322],\n",
       "       [0.99315506],\n",
       "       [0.89855814],\n",
       "       [0.99770504],\n",
       "       [0.9949767 ],\n",
       "       [0.2781296 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.03412073490814,\n",
       " 'precision': 0.7711671866902318,\n",
       " 'recall': 0.7703412073490814,\n",
       " 'f1': 0.7686901866564684}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_result = calculate_results(val_labels, model_3_preds)\n",
    "model_3_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Bidirectional RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_4/20240118-145042\n",
      "Saving model checkpoints to: checkpoints/08_model_4/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 30ms/step - loss: 0.5127 - accuracy: 0.7470 - val_loss: 0.4686 - val_accuracy: 0.7808\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.3111 - accuracy: 0.8737 - val_loss: 0.5121 - val_accuracy: 0.7651\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.2134 - accuracy: 0.9191 - val_loss: 0.5764 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1507 - accuracy: 0.9475 - val_loss: 0.6291 - val_accuracy: 0.7730\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.1048 - accuracy: 0.9637 - val_loss: 0.8398 - val_accuracy: 0.7585\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
    "# x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_4 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_4.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_4 = model_4.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_4\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_4\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.0838463e-02],\n",
       "       [5.6173795e-01],\n",
       "       [9.9640393e-01],\n",
       "       [1.0960973e-01],\n",
       "       [6.8185700e-04],\n",
       "       [9.9276817e-01],\n",
       "       [5.0994414e-01],\n",
       "       [9.9913341e-01],\n",
       "       [9.9856335e-01],\n",
       "       [2.0979431e-01]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.8530183727034,\n",
       " 'precision': 0.7625739943565455,\n",
       " 'recall': 0.7585301837270341,\n",
       " 'f1': 0.7549664582215014}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_result = calculate_results(val_labels, model_4_preds)\n",
    "model_4_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Cov1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_5/20240118-145110\n",
      "Saving model checkpoints to: checkpoints/08_model_5/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 19ms/step - loss: 0.5463 - accuracy: 0.7262 - val_loss: 0.4693 - val_accuracy: 0.7730\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.3309 - accuracy: 0.8629 - val_loss: 0.4812 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.2026 - accuracy: 0.9288 - val_loss: 0.5793 - val_accuracy: 0.7861\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.1314 - accuracy: 0.9540 - val_loss: 0.6393 - val_accuracy: 0.7717\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.0960 - accuracy: 0.9688 - val_loss: 0.6983 - val_accuracy: 0.7677\n",
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.77165354330708,\n",
       " 'precision': 0.768489862704666,\n",
       " 'recall': 0.7677165354330708,\n",
       " 'f1': 0.7660466459325422}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_5 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_5.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_5 = model_5.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_5\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_5\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_5_preds = tf.squeeze(tf.round(model_5.predict(val_sentences)))\n",
    "model_5_result = calculate_results(val_labels, model_5_preds)\n",
    "model_5_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: TensorFlow Hub Pretrained Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       "array([[-0.03133017, -0.06338633, -0.01607502, ..., -0.0324278 ,\n",
       "        -0.04575741,  0.05370455],\n",
       "       [ 0.05080861, -0.01652432,  0.01573779, ...,  0.00976656,\n",
       "         0.0317012 ,  0.01788118]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = hub.load(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\"\n",
    ")\n",
    "embeddings = embed(\n",
    "    [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"I am a sentence for which I would like to get its embedding\",\n",
    "    ]\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_6/20240118-153133\n",
      "Saving model checkpoints to: checkpoints/08_model_6/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 29s 126ms/step - loss: 0.4976 - accuracy: 0.7832 - val_loss: 0.4538 - val_accuracy: 0.8031\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 12s 54ms/step - loss: 0.4171 - accuracy: 0.8136 - val_loss: 0.4457 - val_accuracy: 0.8136\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 10s 45ms/step - loss: 0.4067 - accuracy: 0.8192 - val_loss: 0.4424 - val_accuracy: 0.8110\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 9s 44ms/step - loss: 0.4020 - accuracy: 0.8231 - val_loss: 0.4496 - val_accuracy: 0.8058\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 9s 41ms/step - loss: 0.3993 - accuracy: 0.8267 - val_loss: 0.4444 - val_accuracy: 0.8136\n",
      "24/24 [==============================] - 3s 115ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.36482939632546,\n",
       " 'precision': 0.8152128321955759,\n",
       " 'recall': 0.8136482939632546,\n",
       " 'f1': 0.8123558654721007}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=False,\n",
    "    name=\"USE\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "], name=\"model_6_USE\")\n",
    "\n",
    "model_6.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_6 = model_6.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_6\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_6\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_6_preds = tf.squeeze(tf.round(model_6.predict(val_sentences)))\n",
    "model_6_result = calculate_results(val_labels, model_6_preds)\n",
    "model_6_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256830721 (979.73 MB)\n",
      "Trainable params: 32897 (128.50 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7: TransferLearning with 10% of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_10 = train_df[[\"text\", \"target\"]].sample(frac=0.1, random_state=0)\n",
    "# len(train_data_10)\n",
    "\n",
    "train_data_10_split = int(0.1 * len(train_df))\n",
    "train_sentences_10 = train_sentences[:train_data_10_split]\n",
    "train_labels_10 = train_labels[:train_data_10_split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    444\n",
       "1    317\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_labels_10).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_7/20240118-153309\n",
      "Saving model checkpoints to: checkpoints/08_model_7/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 12s 395ms/step - loss: 0.6513 - accuracy: 0.6938 - val_loss: 0.6180 - val_accuracy: 0.7507\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 6s 246ms/step - loss: 0.5622 - accuracy: 0.7989 - val_loss: 0.5543 - val_accuracy: 0.7690\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 6s 244ms/step - loss: 0.4934 - accuracy: 0.8055 - val_loss: 0.5140 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 5s 224ms/step - loss: 0.4465 - accuracy: 0.8160 - val_loss: 0.4947 - val_accuracy: 0.7795\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 6s 269ms/step - loss: 0.4137 - accuracy: 0.8265 - val_loss: 0.4868 - val_accuracy: 0.7808\n",
      "24/24 [==============================] - 5s 173ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.08398950131233,\n",
       " 'precision': 0.7813169326988488,\n",
       " 'recall': 0.7808398950131233,\n",
       " 'f1': 0.7795856165810638}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\",\n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=False,\n",
    "    name=\"USE\",\n",
    ")\n",
    "\n",
    "model_7 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "], name=\"model_7_USE\")\n",
    "\n",
    "model_7.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_7 = model_7.fit(\n",
    "    x=train_sentences_10,\n",
    "    y=train_labels_10,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_7\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_7\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_7_preds = tf.squeeze(tf.round(model_7.predict(val_sentences)))\n",
    "model_7_result = calculate_results(val_labels, model_7_preds)\n",
    "model_7_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m all_model_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0_baseline\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_0_result,\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1_dense\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmodel_1_result\u001b[49m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2_LSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_2_result,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3_GRU\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_3_result,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4_Bidirectional\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_4_result,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5_Conv1D\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_5_result,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6_USE\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_6_result,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7_USE_10_percent\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_7_result,\n\u001b[1;32m     11\u001b[0m     }\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m     13\u001b[0m all_model_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_model_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     14\u001b[0m all_model_results \u001b[38;5;241m=\u001b[39m all_model_results\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_1_result' is not defined"
     ]
    }
   ],
   "source": [
    "all_model_results = pd.DataFrame(\n",
    "    {\n",
    "        \"0_baseline\": model_0_result,\n",
    "        \"1_dense\": model_1_result,\n",
    "        \"2_LSTM\": model_2_result,\n",
    "        \"3_GRU\": model_3_result,\n",
    "        \"4_Bidirectional\": model_4_result,\n",
    "        \"5_Conv1D\": model_5_result,\n",
    "        \"6_USE\": model_6_result,\n",
    "        \"7_USE_10_percent\": model_7_result,\n",
    "    }\n",
    ").transpose()\n",
    "all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"] / 100\n",
    "all_model_results = all_model_results.sort_values(\"accuracy\", ascending=False)\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_model_results\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\u001b[38;5;241m.\u001b[39mlegend(bbox_to_anchor\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m));\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_model_results' is not defined"
     ]
    }
   ],
   "source": [
    "all_model_results.plot.bar(figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 7s 109ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.2265689999985625, 9.48368635170415)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper_funcs import pred_timer\n",
    "\n",
    "model_6_total_time, model_6_time_per_pred = pred_timer(model_6, val_sentences)\n",
    "model_6_total_time, model_6_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04147825000109151, 0.05443339895156366)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_total_time, model_0_time_per_pred = pred_timer(model_0, val_sentences)\n",
    "model_0_total_time, model_0_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_8/20240118-183630\n",
      "Saving model checkpoints to: checkpoints/08_model_8/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 16s 52ms/step - loss: 0.5074 - accuracy: 0.7532 - val_loss: 0.4631 - val_accuracy: 0.7874\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 6s 29ms/step - loss: 0.3211 - accuracy: 0.8662 - val_loss: 0.5171 - val_accuracy: 0.7651\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 26ms/step - loss: 0.2229 - accuracy: 0.9164 - val_loss: 0.6104 - val_accuracy: 0.7690\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1539 - accuracy: 0.9421 - val_loss: 0.9000 - val_accuracy: 0.7507\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1114 - accuracy: 0.9545 - val_loss: 1.1009 - val_accuracy: 0.7507\n",
      "24/24 [==============================] - 1s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.06561679790026,\n",
       " 'precision': 0.7555035947814924,\n",
       " 'recall': 0.7506561679790026,\n",
       " 'f1': 0.7464335197069463}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
    "x = tf.keras.layers.LSTM(64)(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_8 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_8.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_8 = model_8.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_8\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_8\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_8_preds = tf.squeeze(tf.round(model_8.predict(val_sentences)))\n",
    "model_8_result = calculate_results(val_labels, model_8_preds)\n",
    "model_8_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_9/20240118-183717\n",
      "Saving model checkpoints to: checkpoints/08_model_9/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 11s 38ms/step - loss: 0.5095 - accuracy: 0.7494 - val_loss: 0.4543 - val_accuracy: 0.7874\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.3107 - accuracy: 0.8692 - val_loss: 0.4783 - val_accuracy: 0.7822\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.2076 - accuracy: 0.9206 - val_loss: 0.5974 - val_accuracy: 0.7822\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1544 - accuracy: 0.9445 - val_loss: 0.7350 - val_accuracy: 0.7533\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1026 - accuracy: 0.9600 - val_loss: 0.7264 - val_accuracy: 0.7651\n",
      "24/24 [==============================] - 1s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.50918635170603,\n",
       " 'precision': 0.7648238707003729,\n",
       " 'recall': 0.7650918635170604,\n",
       " 'f1': 0.7642522657439998}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.LSTM(64)(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_9 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_9.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_9 = model_9.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_9\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_9\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_9_preds = tf.squeeze(tf.round(model_9.predict(val_sentences)))\n",
    "model_9_result = calculate_results(val_labels, model_9_preds)\n",
    "model_9_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_10/20240118-183750\n",
      "Saving model checkpoints to: checkpoints/08_model_10/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 34ms/step - loss: 0.5934 - accuracy: 0.6846 - val_loss: 0.4762 - val_accuracy: 0.7795\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3490 - accuracy: 0.8570 - val_loss: 0.4593 - val_accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.2029 - accuracy: 0.9260 - val_loss: 0.5332 - val_accuracy: 0.7808\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1181 - accuracy: 0.9610 - val_loss: 0.5748 - val_accuracy: 0.7795\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.0815 - accuracy: 0.9721 - val_loss: 0.6518 - val_accuracy: 0.7677\n",
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.77165354330708,\n",
       " 'precision': 0.7677367335034279,\n",
       " 'recall': 0.7677165354330708,\n",
       " 'f1': 0.766597011860758}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_10 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_10.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_10 = model_10.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_10\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_10\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_10_preds = tf.squeeze(tf.round(model_10.predict(val_sentences)))\n",
    "model_10_result = calculate_results(val_labels, model_10_preds)\n",
    "model_10_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_11/20240118-183843\n",
      "Saving model checkpoints to: checkpoints/08_model_11/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 29ms/step - loss: 0.5327 - accuracy: 0.7374 - val_loss: 0.4713 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3031 - accuracy: 0.8788 - val_loss: 0.5030 - val_accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.1665 - accuracy: 0.9456 - val_loss: 0.5970 - val_accuracy: 0.7822\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.1048 - accuracy: 0.9691 - val_loss: 0.6389 - val_accuracy: 0.7756\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0759 - accuracy: 0.9778 - val_loss: 0.6311 - val_accuracy: 0.7664\n",
      "24/24 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.64041994750657,\n",
       " 'precision': 0.7665895370389821,\n",
       " 'recall': 0.7664041994750657,\n",
       " 'f1': 0.7651213533864446}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_11 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_11.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_11 = model_11.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_11\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_11\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_11_preds = tf.squeeze(tf.round(model_11.predict(val_sentences)))\n",
    "model_11_result = calculate_results(val_labels, model_11_preds)\n",
    "model_11_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_12/20240118-183924\n",
      "Saving model checkpoints to: checkpoints/08_model_12/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 31ms/step - loss: 0.5557 - accuracy: 0.7097 - val_loss: 0.4800 - val_accuracy: 0.7848\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3321 - accuracy: 0.8631 - val_loss: 0.5126 - val_accuracy: 0.7717\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.1927 - accuracy: 0.9283 - val_loss: 0.5867 - val_accuracy: 0.7822\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.1046 - accuracy: 0.9648 - val_loss: 0.8008 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0651 - accuracy: 0.9755 - val_loss: 0.8304 - val_accuracy: 0.7625\n",
      "24/24 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.24671916010499,\n",
       " 'precision': 0.7629611993882945,\n",
       " 'recall': 0.7624671916010499,\n",
       " 'f1': 0.7608791530897157}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_12 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_12.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_12 = model_12.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_12\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_12\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_12_preds = tf.squeeze(tf.round(model_12.predict(val_sentences)))\n",
    "model_12_result = calculate_results(val_labels, model_12_preds)\n",
    "model_12_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_13/20240118-184002\n",
      "Saving model checkpoints to: checkpoints/08_model_13/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 9s 35ms/step - loss: 0.5318 - accuracy: 0.7330 - val_loss: 0.4711 - val_accuracy: 0.7703\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.3196 - accuracy: 0.8685 - val_loss: 0.5073 - val_accuracy: 0.7690\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.1890 - accuracy: 0.9317 - val_loss: 0.5952 - val_accuracy: 0.7808\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1132 - accuracy: 0.9632 - val_loss: 0.7103 - val_accuracy: 0.7808\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0707 - accuracy: 0.9743 - val_loss: 0.9188 - val_accuracy: 0.7585\n",
      "24/24 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.8530183727034,\n",
       " 'precision': 0.7622010920991931,\n",
       " 'recall': 0.7585301837270341,\n",
       " 'f1': 0.7551340463755609}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_13 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_13.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_13 = model_13.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_13\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_13\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_13_preds = tf.squeeze(tf.round(model_13.predict(val_sentences)))\n",
    "model_13_result = calculate_results(val_labels, model_13_preds)\n",
    "model_13_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_14/20240118-184031\n",
      "Saving model checkpoints to: checkpoints/08_model_14/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 15s 63ms/step - loss: 0.5279 - accuracy: 0.7352 - val_loss: 0.4954 - val_accuracy: 0.7677\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 8s 35ms/step - loss: 0.3030 - accuracy: 0.8822 - val_loss: 0.5241 - val_accuracy: 0.7822\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 7s 32ms/step - loss: 0.1570 - accuracy: 0.9454 - val_loss: 0.6423 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 6s 30ms/step - loss: 0.0875 - accuracy: 0.9680 - val_loss: 0.8196 - val_accuracy: 0.7625\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 6s 28ms/step - loss: 0.0558 - accuracy: 0.9777 - val_loss: 0.9956 - val_accuracy: 0.7651\n",
      "24/24 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.50918635170603,\n",
       " 'precision': 0.7752788386481628,\n",
       " 'recall': 0.7650918635170604,\n",
       " 'f1': 0.7594804747767185}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=512,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_14 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_14.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_14 = model_14.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_14\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_14\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_14_preds = tf.squeeze(tf.round(model_14.predict(val_sentences)))\n",
    "model_14_result = calculate_results(val_labels, model_14_preds)\n",
    "model_14_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorboard/08_model_15/20240118-184152\n",
      "Saving model checkpoints to: checkpoints/08_model_15/checkpoint.ckpt\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 15s 63ms/step - loss: 0.5207 - accuracy: 0.7383 - val_loss: 0.4571 - val_accuracy: 0.7835\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.3062 - accuracy: 0.8809 - val_loss: 0.4732 - val_accuracy: 0.7861\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 7s 34ms/step - loss: 0.1733 - accuracy: 0.9375 - val_loss: 0.6060 - val_accuracy: 0.7612\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 7s 32ms/step - loss: 0.0943 - accuracy: 0.9663 - val_loss: 0.7972 - val_accuracy: 0.7690\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 7s 31ms/step - loss: 0.0597 - accuracy: 0.9759 - val_loss: 0.9504 - val_accuracy: 0.7454\n",
      "24/24 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 74.54068241469817,\n",
       " 'precision': 0.7463476580352829,\n",
       " 'recall': 0.7454068241469817,\n",
       " 'f1': 0.7456923847878935}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = 25\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=512,\n",
    "    input_length=max_length,\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_15 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_15.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history_15 = model_15.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[\n",
    "        create_tensorboard_callback(\"tensorboard\", \"08_model_15\"),\n",
    "        create_checkpoint_callback(\"checkpoints\", \"08_model_15\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model_15_preds = tf.squeeze(tf.round(model_15.predict(val_sentences)))\n",
    "model_15_result = calculate_results(val_labels, model_15_preds)\n",
    "model_15_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
